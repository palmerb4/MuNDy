Dev-facing interfaces

Methodology change: 
Mundy will leverage STK's template abstractions to limit complexity by accessing default fields via string or ID. This seems like a small change, but it means that we generate all fields at run-time, not compile time. The previous design use getters to access fields, resulting in a needlessly complex interface due too the limitations of coompile-time abstractions. With run-time field generation, one of our core data structures will be that of a metaData builderâ€”a class that takes in a hierarchical parameter file that outlines the part structure and the part fields. The builder will use this file to call its own public api for adding parts, subparts, and fields; the generated groups will then be accessible using the names from the parameter file. To enable this type of initialization, fields will be restricted to bool, int, int64, float, double, complex64, complex128. 

Functions will take in Teuchos::ParameterList to specify their fields.


- Particles are marked for deletion and then removed later
- Atomic operations are available for parallel lock free addition and deletion of particles
- Consider adding temporaries; temporary fields on the particles, which can be written to as needed.  
- Do we need sorting?
- Do we need repartitioning vs partitioning? 


This is the namespace/class/function hierarchy:
mundy::
  meta::  (meta programming methods and pre-commit routines) 
    MetaMethod  (interface that forces all mundy methods to output their required fields/part parameters. most of these params will come from the standardized defaults that we produce.) 
    MetaFieldParams  (contains the necessary information to build a field)
    MetaPartParams  (contains the necessary information to build a part with known fields)
    MetaHierarchyParams  (contains the necessary information to build a hierarchy of parts and fields)
    MetaHierarchyBuilder  (constructs the group hierarchy and assigns fields. the user must specify which groups each method will be used with)
  utils::  (extension/streamlining of STK's functionality)
      generate_new_entities_in_part  (Unsure of the best way to set this up. Returning new entities within a new entities part works but requires some effort on the user side) 
      delete_entities_in_part  (Unsure of the best way to set this up. Returning new entities within a new entities part works but requires some effort on the user side) 
      attach_to_node  (make it easier for constraints to connect to particles)
      detach_from_node  (make it easier for constraints to disconnect from particles)
  particle::
    MetaDefaultParticlePartParams
    MetaDefaultSpherePartParams
    MetaDefaultSpherocylinderPartParams
    MetaDefaultEllipsoidPartParams
    MetaDefaultPolytopePartParams
    
  constraint::
    MetaDefaultConstraintPartParams
    MetaDefaultSpringPartParams
    MetaDefaultTorsionalSpringPartParams
    MetaDefaultBallJointPartParams
    MetaDefaultHingePartParams
  
   methods::  
    MethodComputeAABB  (needs to accept a buffer parameter to allow the collision detection method to enlarge the AABB, if necessary. must contain a factory-esk design for computing aabb's for different particles/constraints, e.g., MethodComputeAABB::spheres(sphere_group, aabb_field))
    MethodComputeOBB  (needs to accept a buffer parameter to allow the collision detection method to enlarge the OBB, if necessary. must contain a factory-esk design for computing aabb's for different parts, e.g., MethodComputeOBB::spheres(sphere_group, obb_field))
    MethodDetectNeighbors  (sub-stategies: AABB and bounding sphere, needs a flag for periodicity)
    MethodRefineNeighbors  (sub-stategies: OBB)
    
    MethodGenerateCollisionConstraints 
    MethodResolveConstraints  (sub-stategies: smooth and nonsmooth)
    MethodResolveConstraintsSmooth  (sub-stategies: potential and penalty)
    MethodResolveConstraintsNonSmooth  (sub-stategies: LCP and ReLCP)
    MethodSolveLCP  (sub-stategies: APGD and BBPGD)
    MethodSolveReLCP
    
    MethodTimeIntegration  (sub-stategies: NodeEuler, NodeAdamsBatchford)
    
    MethodComputeMobility  (I'm very unsure how to set this up for multi-species support)
       
       
All of Mundy + strategy implementations    
mundy::
  meta::
    FieldParams
    PartParams
    MetaMethod
    HierarchyParams
    HierarchyBuilder
       
  methods:: 
    ComputeAABB
      ComputeAABBSphereVariant 
      ComputeAABBSpherocylinderVariant
      ComputeAABBSuperEllipsoidVariant
      ComputeAABBPolytopeVariant
      ComputeAABBCollisionVariant
      ComputeAABBSpringVariant
      ComputeAABBTorsiionalSpringVariant
      ComputeAABBJointVariant
      ComputeAABBHingeVariant

    ComputeOBB
      ComputeOBBSphereVariant
      ComputeOBBSpherocylinderVariant
      ComputeOBBSuperEllipsoidVariant
      ComputeOBBPolytopeVariant
      ComputeOBBCollisionVariant
      ComputeOBBSpringVariant
      ComputeOBBTorsiionalSpringVariant
      ComputeOBBJointVariant
      ComputeOBBHingeVariant
    
    ComputeBoundingSphere
      ComputeBoundingSphereSphereVariant
      ComputeBoundingSphereSpherocylinderVariant
      ComputeBoundingSphereSuperEllipsoidVariant
      ComputeBoundingSpherePolytopeVariant
      ComputeBoundingSphereCollisionVariant
      ComputeBoundingSphereSpringVariant
      ComputeBoundingSphereTorsiionalSpringVariant
      ComputeBoundingSphereJointVariant
      ComputeBoundingSphereHingeVariant
    
    SolveLCP
      LCPSolverAPGDTechnique
      LCPSolverBBPGDTechnique

    // neighbor methods
    DetectNeighbors
      DetectNeighborsAABBTechnique
      DetectNeighborsBoundingSphereTechnique
      
    GhostNeighbors
    
    RefineNeighbors
      RefineNeighborsOBBTechnique
    
    // constraint methods  
    GenerateCollisionConstraints
      GenerateCollisionConstraintsSpherocylinderVariant
      GenerateCollisionConstraintsSuperEllipsoidVariant
      GenerateCollisionConstraintsPolytopeVariant

    ComputeConstraintViolation
      ComputeConstraintViolationCollisionVariant
      ComputeConstraintViolationSpringVariant
      ComputeConstraintViolationTorsiionalSpringVariant
      ComputeConstraintViolationJointVariant
      ComputeConstraintViolationHingeVariant

    ComputeConstraintProjection
      ConstraintProjectionCollisionVariant
      ConstraintProjectionSpringVariant
      ConstraintProjectionTorsiionalSpringVariant
      ConstraintProjectionJointVariant
      ConstraintProjectionHingeVariant

    ComputeConstraintForcing
      ConstraintForcingSpringVariant
      ConstraintForcingTorsiionalSpringVariant
      ConstraintForcingJointVariant
      ConstraintForcingHingeVariant

    ComputeConstraintViolationLinearizedRateOfChange
      ConstraintViolationRateOfChangeCollisionVariant
      ConstraintViolationRateOfChangeSpringVariant
      ConstraintViolationRateOfChangeTorsiionalSpringVariant
      ConstraintViolationRateOfChangeJointVariant
      ConstraintViolationRateOfChangeHingeVariant
    
    ComputeConstraintJacobian (dt D^T M D)
    
    ResolveConstraints
      ResolveConstraintsSmoothPotentialTechnique
      ResolveConstraintsSmoothPenaltyTechnique
      ResolveConstraintsNonSmoothLCPTechnique
      ResolveConstraintsNonSmoothReLCPTechnique
    
    // motion methods
    TimeIntegration
      TimeIntegrationNodeEulerTechnique
      TimeIntegrationNodeAdamsBatchfordTechnique
    
    ComputeMobility
      NodeEulerTechnique

User facing interfaces    
mundy::
  meta::
    Method
    FieldParams
    PartParams
    HierarchyParams
    HierarchyBuilder
  methods::  
    DetectNeighbors.
      ComputeAABB  (one for each particle/constraint type)
    GhostNeighbors
    RefineNeighbors.
      ComputeOBB  (one for each particle/constraint type)
    GenerateCollisionConstraints
    ResolveConstraints
    TimeIntegration  (one for each particle (constraint?) type)
    ComputeMobility  (one for the entire particle hierarchy)





GroupHierarchyParams:
  int group_id;
  std::string group_name;
  std::topology group_topology;
  std::vector<FieldParams> group_fields;
  std::vector<GroupHierarchyParams>;
  
The above allows a tree/YAML-esk hierarchy of parameters
GroupHierarchyParams:
  - group_id: 100
    group_name: "particle"
    group_topology: stk::topology::ENTITY
    group_field_params:
      - field_name: "force"
        field_rank: std::topology::ENTITY_RANK
        field_type: 
    
  std::vector<GroupHierarchyParams>;



We offer certain shapes and certain constraints. The fields within these shapes are extensible but the design of these shapes is fixed. A part of spheres will always have the same topology. Shapes/constraints should be stored in separate parts and loops should be done over these parts to speed up computation. Each shape must have a method for computing its AABB, bounding sphere, and OBB. If computing the OBB is too difficult, return the AABB. Each shape should also have a means of computing its minimal distance between any other shape. Each constraint must have a method for computing its constraint violation and its induced force/torque. If ComputeAABB was given A part and A mundy type, then we could deduce which method to use. If each part in the hierarchy also has a mundy type associated with it, then type deduction (given the entire hierarchy) is possible. However, a part in the hierarchy need not have a mundy type. 

Why not wrap BulkData and MeshBuilder, so that we automatically create a part for each shape. Consider using the same functionality as m_topologyPartMap (a map of topology and part) and m_partTopologyVector (a vector of topologies). Given the WrappedBulkData, we can access the sphere part using something similar to metaData.get_topology_root_part(stk::topology::PARTICLE). That brings us back to wrapping stk to create a way of storing mundy::multibody. This way we can get an iterator over the current shapes or constraints. 
   
We're using the factory concept with mundy::multibody as the enum for the factory. This means that ComputeAABB should output an std::map<mundy::multibody, std::unique_ptr<PartParams>>. So each of mundy's methods are specilized for our constraints and shapes.

What if a method, like ComputeAABB, should work for particles or constraints? Then mundy::constraint and mundy::particle should come from the same base type: mundy::multibody. ComputeAABB will take in a part and a vector of mundy::multibodys, loop over each part coresponding to the provided mundy::multibodys and check if the provided part and that part intersect. If so, it will call the corresponding factory routine. There's a problem. What if I don't want to perform neighbor detection with all spheres? Maybe only RedSpheres should have neighbor detection. That's addressed by the current ComputeAABB design, but not by the current Hierarchy design. Maybe something like add_multibody_method_to_part and add_multibody_method_to_all_parts_of_type. AABBFactory is a private class within ComputeAABB. This will allow users to directly call the factory methods from ComputeAABB, if they so choose. 

I'm struggling with how the particles/constraints perform very fine-grain activities like finding the minimal separation distance, constraint violation, rate of change of constraint violation, constraint projection, constraint forcing. These are not fixed for a single particle/constraint type and the method used to solve them needs to be able to vary simulation to simulation. For example, I may want all of my constraints to use a different flavor of projection. This would mean that the one creates a Varient of the core multibody types, assigns to that type a Style for each of the required operations or (if they so choose) no style at all if that operation will not be used. Each style will have its own requirements. Ahhhhhhh. But some methods require certain operations to be defined or worse, what if a new method requires an operation that we havn't specified. This won't work. Alternatively, we replace our AbstractFactory with a FactoryRegistory that contains a map from identifier to factory creator. This will allow our factories to accept any number of Managers. Each Manager should have a uniform interface from which it is derived.

I want to be able to use the functionality provided by MetaMethodFactory for all of my factories; however, its static member variables make inheritance impossible. Now, we could use CRTP to instaniate independent factories for each MetaMethod and this seems tempting. I just don't like the complexity of registering with more than one factory

ComputeAABB is a MetaMethod, it may also contain multiple MetaTechnique which abstract independent places in the method for quick multiple techniques could be used. These techniques will have their own requirements (just like a MetaMethod) and will be chosen based on the input params. This concept works is logical based on the factory design Factory -> Method -> Techniques. Even if a method requires another method, then that sub-method should be it's own method available to the public. Come on, a technique is just a method with a longer identifier. No, not necessarily. We don't want the exponential increase in methods caused by needed to implement every purmutation of independent tequniques as a single method. It also clutters our MetaMethodFactory's output. Instead, every set of techniques that correspond to different ways to perform the same task should have a factory. This factory should be set up just like MetaMethodFactory except with CRTP based on the task being carried out. Then again, maybe not. Is it ok for tasks like computing the minimal separation distance, constraint violation, rate of change of constraint violation, constraint projection, or constraint forcing to be carried out as loops that employ a different style? That is, can the techniques look like MetaMethods? I think so! Techniques need to be able to differ from multibody to multibody, thereby requiring an extra layer in out design hierarchy. Factory -> Method -> Variant -> Technique speaks to me. Variants will only be used to discribe the difference in a method from multibody to multibody. Note, some Variants will be for sets of multibodies, such as a mobility matrix that only works for spheres and rods. Each varient may have its own set of Techniques. Any Method, Variant, or Technique can be swapped out allowing for multi-scale code extensions based on user needs. What's the best way to specify techniques? In the code, this will be done as follows:

Each DerivedMetaMethod is registered with MetaMethodFactory. 
Each Varient is registered to MetaVarientFactory<DerivedMetaMethod>. 
Each Technique is registered to MetaTechniqueFactory<MetaVarientFactory<DerivedMetaMethod>>> or MetaVarientFactory<DerivedMetaMethod> depending on if they are associated with a variant or not. 
This registry hierarchy will work well in the future when building an API with drop down windows. 

What if a technique requires techniques or if you want a technique to have variants? If you ask this question, then you need to flip your design structure. Instead of a technique with vartiants for each multibody type, try a method with varients for each multibody type each with a unique implementation of your technique. Another question, what if I want to assign a certain method with varients to the parent particle object and have all of the subparts automatically inherit the proper vartiant? Perfectly valid question, then we should add a Varient flag to our API that says Automatic. That's going to be hard to ptogram! Actually, idk if it's possible or if it's recommended since some variants may not have the same techniques and because subparts have nothing that identifies them as a sphere or rod. If the part has the required topology and the right fields then it can be a sphere or a ellipsoid.  

There is an issue. String lookup has a cost, so it's not a good idea to have ALL of mundy's methods and techniques flattened out (it also leads to needlessly complex names). I find it best to have methods be hierarchical rather than flat. Let's set things in stone. A Varient is a specialization of a method for a certain multibody type and is registered to MetaVarientFactory<DerivedMetaMethod>. A Technique is a specialization of either a Method or a Varient and is registered to MetaTechniqueFactory<MetaVarientFactory<DerivedMetaMethod>>> or MetaVarientFactory<DerivedMetaMethod>. Ahhhhhh. What if someone else wants to use a method where a technique is required, as can easily arrise if a technique becomes useful elsewhere? The parameter will say be associated with the wrong factory and will be inaccessible. This means that methods cannot be used where a technique is required. Something that is a technique is forever bound to being a technique. That is a bad design. This is a major reason to not use the technique design where techniques are bound to methods. It seems like we could use modules as collections of similar methods. At least that would limit the number of methods being searched. The clutter and complex names issue still persist. Why should a heavily specialized technique be shown to the public as a method. It shouldn't. Techniques (this is a naming hierarchy) is a good idea. The only problem is that methods should be able to be used in place of techniques. This comes down to registration. Maybe we specify both a name and a registary within which to search for the method. That makes it more explicit. 

Create a class type which represents all techniques that should achieve the same task. Because they all achieve the same task, I see no reason why a method would be used in their stead. That's axactly what we currently have. No, no it is not. It's every so slightly different. There are methods that require multiple techniques. Those techniques should not be associated with that class but with a type that identifies them as techniques for performing some action... some method. We're back to the three-level hierarchy of Factory -> Method -> Varient -> Technique. Varients are just Techniques. By using strings instead of enums, we allow our users to add their own particles. I think that's fair. Honestly, I think we should use the words Method/Varient/Technique but only use MetaMethod types. Otherwise we risk users thinking that Varient or Technique registeries with the same type are separate when they aren't.

MetaMethodFactory<MetaMethodFactory<DerivedMetaMethod>>>

!!Need a requirement conflict resolver!!
!!Our PartParams needs an && operator to merge them together!!
!!What if I want to combine multiple independent computations? Can the compute routines have an overloaded && operator for merging their lambdas!!
!!What if a constraint has multiple lagrange multiplies like friction!! 




Ok, now we can start worrying about the HierarchyBuilder. What are the key pieces of functionalitry that it needs to offer?
1. Build Hierarchy from yaml file
2. Build Hierarchy from api
  2.1. Add new part
  2.2. Declare subpart
  2.3. Declare field
  2.4. Declare method
3. 
  
What functionality should PartParams offer? 
1. A merge function for merging params. ONLY merge if the number of states is different.
2. An invarient checker to ensure that the part params are currently valid.
3. Consistant assert statements that the attempted modifications are valid. 


How to handle field and part names? For fields, there should exist default names. If the parameter file does not specify a name, then get_part_requirements/get_valid_params will use the default name. This ensures name consistancy. Parts are different. Methods can work on any part that has the right topology and the right fields; part names don't matter. So then why should part params require that I set a default name. It creates a weird duality. Merging fields requires that they have the same name and rank, but merging parts often doesn't. For both FieldParams and PartParams. The merge operation does NOT require the two have the same name. 

Ahhhhhhh. The use of a map from field name to field params is wrong since the name may be nonunique if a force field is applied to nodes and elements. Just use a vector of maps, with one map per rank. 

The upcoming MetaKernel class:
 - Need to have different kernel classes based on the input. For example, computing the minimum separation distance takes in two entities and their multibody type takes in four parameters.  
 - Can be initialized outside of a kokkos loop but can be executed inside. This is difficult since the varius kernels need initilized outside of the loop but chosen inside the loop. The setup function will need to initialize a kokkos map from multibody type to initilized kernel. 
 - 
 
 
Summation of the following: We will not use a multibody type and will stick with strings like in the varient design. We will use Kokkos's map class to perform the lookup. 
 
The problem I have with creating a user-extensible MultiBody kernel class is that we don't want to perform string lookups within the GPU loop. How to create a user-entensible multibody type identifier? How to take in multibody type itendifier and go from that to a method corresponding to that ID. MetaMethodFactory has a map from string to creation routines, can we get a MultibodyKernelFactory with a map from pair of type identidiers to collision detection routine? std::map isn't GPU compatable... mundy::multibody needs set in stone. Users will not be able to add new multibody types without directly modifying mundy. Users will, however, be able to swap out methods that act on these multibody types?  
 
What methods depend on the multibody type?
 - ComputeAABB
 - ComputeOBB
 - ComputeBoundingSphere
 - GenerateCollisionConstraints
 - ComputeConstraintViolation
 - ComputeConstraintProjection
 - ComputeConstraintForcing
 - ComputeConstraintViolationLinearizedRateOfChange
 - TimeIntegration
 - ComputeMobility
 
Which methods have non-trivial looping over different types in isolation. 
 - GenerateCollisionConstraints (requires looping over the connectivity of each entity to compute the minimum separation distance between them)
 - ComputeConstraintViolationLinearizedRateOfChange (requires looping over the connectivity of each entity to get the contact point and force vector of the attached constraint)

What's the most efficient way to decide which method to use? When possible, we want to perform loops over a single multibody type at a time. When a single part can be used, the user can simply pass in the part and its multibody type. For systems that don't know the explicit multibody type beforehand, the part needs to store its multibody type as a field. The question becomes, how to quickly go from that field to the corresponding method? Kokkos has an unordered and an ordered map. We could just use string lookup and return to allowing the users to generate their own multibody types. I prefer that. 

How does one build a kernel outside of a kokkos loop and then use it inside the loop? I believe nalu::Kernel addresses this by having the shared memory required by the kernel constructed outside the loop.  


MetaKernel 



Lessons from nalu. 
 - The functionality offered by the current MetaMethod design is perfect.
 - MetaMethood should have three stages: pre_run, run, and post_run. pre_run is a good chance to store field objects, collect sub-methods, and collect any varients. Maybe setup, run, postprocess?
 - Nalu + stk offer simd optimized loops. Long term, we should do the same. See simd_elem_field_updater 
 - MetaMethod should take in the BulkData and part it will act on in its constructor.
 - ScalarType should be templated out.   
 - Use stl::math for abs/sin/cos etc
 - Need a kernel concept for executing operations within a kokkos loop!
 - A matrix-free linear system solver
 - Nalu allows the user to specify tasks that should be performed on the mesh during preprocessing.
 - Nalu has an advanced periodic manager!
 - howToNgp.cpp shows that looping over connected nodes is allowed!
 - run_face_elem_algorithm optimizes looping over connected nodes connected to a face to a degree almost unfathomable. Check out the chain LowMachEquationSystem.C -> AssembleFaceElemSolverAlgorithm.C -> AssembleFaceElemSolverAlgorithm.h -> KernelBuilder.h -> Kernel.h -> MomentumOpenEdgeKernel.C -> ElemDataRequests.C
 
I'm not a fan of FieldInfo/FieldSet because it needs to make clear that different ranks can have identical fields. Not to mention, the comparisons are faster if you break the field storage into ranks then names. The MasterElement stored in ElemDataRequests is only necessary for FEM. We may want to add support for it once we add in particle/grid coupling, but not now. As a result, ElemDataRequests is just a vector of FieldParams + the fields themselves. Hence, the initialization routine for MomentumOpenEdgeKernel is just setting up the required FieldParams. We already have this with our static requirements! Not to mention, we achive it with a cleaner untuitive design. Kernels will be registered with a Kernel factory and will be associated with the class that needs them. Note, the use of shared and scratch views within MomentumOpenEdgeKernel is a direct result of the nalu's MasterElements. Without MasterElements, there is no need for these shared views. Also without MasterElements we don't need meFC_ or meSCS_. We do want Kernel and NGPKernel except with a custom execute funtion. We can replace KernelBuilder with our auto-registering factory design. Our kernels may take in one element with known multibody type, or two elements with known type. We need to generate the correct kernel based on those types, I do not currently know how. Well, multibody type is a string, so all we really want is a map from a pair of strings to the kernel generators. Then, we can simply replace run_face_elem_algorithm with howToNgp's connectivity loop.

?????What does Nalu use instead of MeshBuilder??????????????????????????????????????????????????

   
DetectNeighbors should loop over each part and compute their AABB field, from there it should use that field to detect neighbors between all parts simultaniously. But how does DetectNeighbors know which compute AABB field method to use if it is only given the particle part. It can't. DetectNeighbors needs to be given the entire hierarchy. But how can DetectNeighbors know which part of the hierarchy corresponds to a sphere? 


All spheres have a radius. This is fact. Why don't we use a type identifier like stk::topology and store it as a field, because that would make required type deduction
   
   
problem I have with storing the fields is that we would need a copy of this class for each multibody
type. We may have a different parameter list per multibody type. We may also have the same parameter list per
multibody type. We simply allow the user to pass in all the parameters for all varients in this constructor!!!
The execute command will then take in the multibody name and use the correct params.

This means that compute AABB should store instances for each multibody name rather than calling
create_new_instance within execute. This also means that the constructor for multibody method needs to be (const
stk::mesh::BulkData *bulk_data_ptr, const Teuchos::ParameterList &parameter_list)


Create the internal parameter list
Each variant has a different set of valid parameters. We could, in theory, collect all of these requirements, and
output it when get_part_requirements is called but doing so could lead to outputting more requirements than
necessary. We need to better tie together how the methods are initialized and how the methods should be
constructed. Currently, we access requirements based on multibody name but only want to initialize one
ComputeAABB. As well, in the current parameter list paradigm, method requirements are hidden within each part.

Some methods, like ComputeAABB, take in one part and act on it in isolation. Technically, these methods could
take in a vector of parts and loop over each of them. Methods like ResolveConstraints, on the other hand, are
unique in that they need to take in a vector of parts and have those parts all interact in unison. Sure, lets
just change the interface to be a map of named parts.

If we consider that Methods assigned to Parts like Fields, then a Method can take in a vector of subparts... No
it cant. If this thought were correct, then how would ResolveConstraints know how to specialize for each
multibody type? I think this shows the flaw, we are trying to consider Methods like Fields, but unlike fields, a
method assigned to a parent, may need to specialize for each of its children. The response may seem to be
assigning the specializations to each subpart, but what about the parent method that needs to control their
collective interaction?
Well, how does ComputeAABB for a sphere differ from ComputeAABB for a Spring? They have different kernels. In the
PointForceFMM technique of ComputeMobility, each part passed into ComputeMobility will differ based on the kernel
they use for computing their imposed force.

The multibody class stores named kernels. We provide some prebuilt kernels.

Every single part that we are given should also provide its own kernel. But what if the method requires many

(Cannot wrap parts in their methods because it isolates methods from their children) Ok, why not wrap a part in
its methods? Allow multiple ComputeAABBs to be created, one for each part that requests that method. That creates
an issue because how would subparts access the methods generated by their parents!

Need to assign kernels to parts and have those parts be able to move around logically. Default kernel can be set,
but the sub-parts can overwrite it. How does this allow subparts to inherit methods? The Hierarchy builder can
handle the passing down of kernels to subparts. Kernels are just strings, so the parameter list can handle this.
How will the HierarchyParams be translated into parameters for the various methods? How will MetaMethods be
initialized only once for all subparts. The only thing that differs from subpart to subpart is the kernels, so if
we initialize the Kernels and the Methods separately, then all is well; there is no need to have a Method
differ from sub-part to sub-part. That does mean that each Kernel needs to be initialized and maintained (just
like each Method), but it seems like the best idea so far. Now, a Method has no ability to be given a Particle
part and from there infer the kernels of its subparts. This induces an odd inheritance structure where the
highest part to request a certain kernel is the only one that needs to generate that kernel. Here's what I
mean, if I define a Kernel and apply it to my Spheres, only one instance of it needs generated and shared among
all sub-spheres.

Methods are given to a parent class and inherited by all subparts. A method consists of an operation that can be
performed over the entire part, or more restrictively, a set of vectors of subparts. Typically this involved the
use of one or more kernels. A kernel is an atomic unit of computation applied to an indivisible number of
elements. A method may request that all sub-parts define a certain kernel. In some cases, a method will define a
default kernel, but it need not. Think of methods like inherited member-functions. Providing a default kernels is
analogous to providing a default instance of a method function, the inherited classes can overwrite that member
function or not. Similarly, not providing a default function is analogous to a pure virtual function. We should
wrap BulkData and MetaData to provide Kernel and Method support. Methods and Kernels are populated on commit.
Fields can be accessed using a known rank and field name. Methods/kernels can be accessed using a known Part and
Method/Kernel name. Just like Fields, methods/kernels are shared among all subparts. A method/kernel will contain
all of its required fields without having to perform string lookup. The string lookup is performed on commit.
Within both a method and a kernel, parameters that do not change the method's required fields can be changed at
any time. Parameters that would change required fields need to be given pre-commit. We should split up the
parameters in this way. NGPMesh transfers fields to the GPU, so to should our wrapper transfer methods to the
GPU.

There is no need to wrap BulkData, just change the constructor here to take in a vector of parts and all of
their params. Kernels are registered with the method that requires them. This could be multiple methods if you so
choose. The Kernels that a specific part will use are specified in the parameter list. Those kernels will be
constructed using the MetaKernelFactory<MetaMethod>::create_new_instance. The actual execute function won't take
in anything.


   
Quaternion
filterOutSelfOverlap
filterOutNonLocalResults
create_ghosting
compute_maximum_abs_projected_sep
compute_diff_dots
generate_neighbor_pairs
generate_collision_constraints
compute_constraint_center_of_mass_force_torque
compute_the_mobility_problem
compute_rate_of_change_of_sep
update_con_gammas
swap_con_gammas
step_euler
resolve_collisions

   
   
   
   
   
   


Things that are missing,

Turn Group into an pure virtual interface. Define inheritance via subgroups. 

1. How to make looping over particles easier? 
	(Done) Need a function for returning a group selector.
	(Done) Need a function that returns the entity bucket.
	
2. How to make reading and writing the mesh easier? 
	Need a function that takes in an stk::io::StkMeshIoBroker and adds the fields to it. Can a parameter list be provided to specify which fields (if any) to output? 
	
3. How to make neighbor detection easier? 
	Need a function for computing/pupulating the aabb field. Need a default aabb field within the particle and constraint group.

4. How to make partitioning more visible to the user?
	Need a function for computing/populating the parallel rank field. Need a default rank field within the particle and coonstraint groups. Make sure it calls this function for its parent class!

5. How to make initializing particles/constraints easier? 
	Need a function within the particle group for asigning a random initial orientation and initial position. Random position should be within some bounds.

6. How should a user pass the mobility op to the constraint resolution function? 

7. How to handle periodicity? 
	Need a function  within the particle and constraint group for applying periodicity to the coordinates.

8. Need a parent class for Particles and Constraints to prevent code duplication and ensure consistency with respect too defaults















    
